---
title: "Practice SEM"
output: html_notebook
---


```{r}

library(lavaan)

dat <- read.csv("https://stats.idre.ucla.edu/wp-content/uploads/2021/02/worland5.csv")

# Lavaan Syntax:
# ~ predict, used for regression of observed outcome to observed predictors (e.g., y ~ x)
# =~ indicator, used for latent variable to observed indicator in factor analysis measurement models (e.g., f =~ q + r + s)
# ~~ covariance (e.g., x ~~ x)
# ~1 intercept or mean (e.g., x ~ 1 estimates the mean of variable x)
# 1* fixes parameter or loading to one (e.g., f =~ 1*q)
# NA* frees parameter or loading (useful to override default marker method, (e.g., f =~ NA*q)
# a* labels the parameter ‘a’, used for model constraints (e.g., f =~ a*q)
# . in front of parameter denotes an endogenous variable

# Covariance / Variance Matrix ----
# With SEM, the goal is to replicate the variance-covariance matrix with specified model parameters
cov(dat)


#SLR using lm() from base R ------
m1a <- lm(read ~ motiv, data=dat)
fit1a <- summary(m1a)
print(fit1a)


# SLR using lavaan ------

m1b <- '
  read ~ 1 + motiv
  motiv ~~ motiv
' 
fit1b <- sem(m1b,data=dat)
summary(fit1b)

# Interpretation: y = read, x = motiv, regression coefficient / gamma = .530, intercept (a, alpha) = 0, residual variance of endogenous variable = 71.766. 

m1b <- '
  read ~ motiv
  motiv ~~ motiv
' 
fit1b <- sem(m1b,data=dat)
summary(fit1b)

# the df's don't change because linear regression models are always saturated. A Saturated Model is where the number of parameters/coefficients is equal to the number of data points. This is like a ‘connect the dots’ model where the line or curve passes through each point. This is considered to be the perfect model as it takes into account all the variance in the data and has the maximum achievable likelihood.

# ML versus least squares ----
# The fixed effects of an ordinary least squares regression is equivalent to maximum likelihood but the residual variance differs between the two.

# Multiple Regression ----
# In matrix notation:
# y1 = alpha1 + vector of exogenous variables * vector of regression coefficients for total number of exo var + residual of endogenous variable
# assumption that mean of residuals is zero and the residuals of endogenous variable y are uncorrelated with exogenous variables in vector

m2 <- '
  read ~ 1 + ppsych + motiv
  ppsych ~~ motiv
'
fit2 <- sem(m2,data=dat)
summary(fit2)

# intepreting output:
# For a one unit increase in negative parent psych, reading score decreases by .275 points. For a one unit increase in motivation, reading score increases by .461 points. Parent psych and motiv have a negative covariance, meaning as one increases, the other decreases and vice versa. Residual variance for read, which is the sum of squared differences between predicted and observed data is 64.708


# Multivariate Regression ----

m3a <- 
  ' read ~ 1 + ppsych + motiv
  arith ~ 1 + motiv
  '
fit3a <- sem(m3a,data=dat)
summary(fit3a)

# interpreting output:
# 39.179 (positive association between the variance of read and arith not accounted for by the exogenous variables.The relationship of read on motiv is 0.476 meaning as student Motivation increases by one unit, Reading scores increase by 0.476 points ***controlling**** for the effects of Negative Parental Psychology. Finally arith on motiv is 0.6; an increase of one point in student Motivation results in a 0.6 increase in Arithmetic scores. Multiple R squared 1 - 65.032 = .34968 so 35% of variance in reading is explained by ppsych and motiv with 65.032 residual variance.


# Multivariate Regression removing default covariances ---

# By default, lavaan will covary redisual variances of endogenous variables.So in the model below, Y1 and Y2's residual variances covary unless the covariance between the two factors is set to zero. Also, by default, the intercepts are implyed but not included in the output for lavaan. Include 1 in output via endogenous ~ 1 + exo ...
# If the covariance is turned on then the residual variance will account for the effect of ppsych on arith that was not modeled. Any coefficients that are not modeled in the other regressions will appear in the residual covariance, making it larger. This means that lavaan is NOT equal to running separate regressions unless covariance is constrained to 0.

m3d <- '
read ~ 1 + ppsych + motiv
arth ~ 1 + motiv
read ~~ 0*arith
'

# Known values, parameters and degrees of freedom----

# df calculation for multivariate regression:

# calculate sample covariance matrix
cov(dat[,c("read","arith","ppsych","motiv")])

# find number of free parameters
# p(p +1)/2 where p is number of items in metric
# therefore, for this model we get 4(4+1)/2 = 10
# in this example, the maximum number of parameters that can be estimated is 10

# example of fixing intercept of arith to zero (alpha subscript 2,1)

m3aa <- '
read ~ ppsych + motiv
arith ~ motiv
ppsych ~~ ppsych
motiv ~~ motiv
ppsych ~~ motiv
'

fit3aa <- sem(m3aa, data=dat)
summary(fit3aa)

# interpretation of results

# number of free parameters is number of unique model parameters - fixed parameters so df is 10  - 9 = 1

# A model is just-identified or saturated if df = 0
# A model is under-identified if known para < free para (bad)
# A model is over-identified if known para > free para (this is ideal). 

#The point of running an structural equation model is to be able to be wrong - and that's only true if it's over-identified (i.e. has degrees of freedom greater than zero).So we want over-identified models, because they provide a single solution that can possibly be wrong. Just identified models also provide a single solution, but it cannot be wrong.

# for example: 

# x+y=5
# x−y=1
# 2x+2y=6
# There are three equations, and two unknowns. It's now possible for this set of equations to be wrong, and it is wrong. But change that value of 6, and the model could be correct and over-identified:

# x+y=5
# x−y=1
# 2x+2y=10

# where free parameters = 2 and known = 5 (known > free = over-identified) and there is one possible solution:

# 3+2 = 5
# 3-2 = 1
# 3*2+2*2=10

# Back to the m3aa model, it is over-identified because there are 10 known, df = 1

# PATH ANALYSIS ----

# residual covariance between reading and arith is not modeled because we're now looking at whether reading predicts arith directly

m4a <-
'
    read ~ 1 + ppsych + motiv
    arith ~ 1 + motiv + read
'
fit4a <- sem(m4a, data=dat)
summary(fit4a)

# MODIFICATION INDEX ----

# A 1 df chi-square test that assess hoee the model chi-square will change as a result of include a specific parameter in the model. Higher chi-square, the bigger the impact of that parameter being added. determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies

# using m4a as an example:

modindices(fit4a,sort=TRUE)

# left-hand side (lhs)
# operation (op)
# right-hand side of the equation (rhs)
# the modification index (mi) which is a 1-degree chi-square statistic
# expected parameter change (epc) represents how much the parameter is expected to change
# sepec.lv standardizes the epc by the latent variable (in this case there are none)
# sepec.all standardizes by both Y and X
# sepc.nox standardizes all but X variables (in this case Y only)

# Interpretation:

# Motivation as endogenous and artimetic as it's exogenous predictor is the strongest but not all index changes make sense because this would result in negative chi-square for model4a, 4.870 - 68.073 = -63.20.

# others are zero, which mean modifications are irrelevant

# here we might run the model again with ppsych as exogeous on endogenous arith so that ppsych predicts both arith and reading in the overall model.

# after modification index

m4b <- '
    read ~ 1 + ppsych + motiv
    arith ~ 1 + motiv + read + ppsych
'
fit4b <- sem(m4b, data=dat)
summary(fit4b, fit.measures=TRUE)

# MODEL FIT STATS:

# Model Fit Statistics
# Modification indexes gives suggestions about ways to improve model fit, but it is helpful to assess the model fit of your current model to see if improvements are necessary. We use model fit when the model is not saturated, which measure how closely the (population) model-implied covariance matrix  matches the (population) observed covariance matrix . SEM is also known as covariance structure analysis, which means the hypothesis of interest is regarding the covariance matrix. Null hypo states that the two matrices are the same, which means that we failed to find that our model is different, meaning it fits with some level of error, not that it is the best model. Alternative states that our pop implied model does not accurately represent the population observed model.

summary(fit4a, fit.measures=TRUE)

# THE MEASUREMENT MODEL----

# if model is under-identified;
# fix the first loading of each factor to 1 
# or
# fix variance of each factor to 1 but freely estimate all loadings

m5a <-
  'risk =~ verbal + ses +ppsych
  verbal ~ 1
  ses ~ 1
  ppsych ~ 1
  '
fit5a <- sem(m5a, data=dat) 
summary(fit5a, standardized=TRUE) 


# STRUCTURAL MODELS ----


  





























```

